{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":420,"sourceType":"datasetVersion","datasetId":19}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"script","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [markdown]\n# This kernel provides an overview of a classic machine learning problem involving the taxonomy of species of the Iris flower by notable statistician R.A Fisher (https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/15227/1/138.pdf), as well as an introduction to the training (and deployment) of machine learning models.\n# \n# Git repo for the web app, implemented with FastAPI, HTML and CSS: https://github.com/adaniel105/iris-classification \n\n# %% [markdown]\n# # Importing Libraries\n# A good handful of libraries are needed to handle and manipulate data for efficient statistical learning. Some of these include; NumPy (array/matrix manipulation), Pandas (flexible data analysis/manipulation tool), matplotlib, seaborn (data visualization) and scikit-learn (provision of tools for machine learning tasks i.e classification, regression) along with multiple other libraries which cater to the more specific needs of data analysts and scientists.\n# \n# We import a few of these for now, importing more of them as we require their use.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-08-02T18:26:30.447450Z\",\"iopub.execute_input\":\"2024-08-02T18:26:30.447983Z\",\"iopub.status.idle\":\"2024-08-02T18:26:31.751559Z\",\"shell.execute_reply.started\":\"2024-08-02T18:26:30.447939Z\",\"shell.execute_reply\":\"2024-08-02T18:26:31.750477Z\"}}\nimport numpy as np\nimport pandas as pd\n\n# %% [markdown]\n# # Loading the Dataset\n# \n# After importing the necessary libraries, we load the dataset using Pandas, and view it with the ```df.head()``` function, which returns (unless specified) the first five rows of the loaded dataset, confirming the success of the operation.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-08-02T18:26:31.753916Z\",\"iopub.execute_input\":\"2024-08-02T18:26:31.754563Z\",\"iopub.status.idle\":\"2024-08-02T18:26:31.812800Z\",\"shell.execute_reply.started\":\"2024-08-02T18:26:31.754518Z\",\"shell.execute_reply\":\"2024-08-02T18:26:31.811295Z\"}}\ndf = pd.read_csv(\"/kaggle/input/iris/Iris.csv\")\ndf.rename(index=df.Id, inplace=True)\ndf.drop(\"Id\", axis=1, inplace=True)\ndf.head()\n\n# %% [markdown]\n# # Data Understanding\n# \n# The step in the machine learning process involves gaining general insights about the data with reference to its structure and quality. We can do this in a number of ways.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-08-02T18:26:31.814382Z\",\"iopub.execute_input\":\"2024-08-02T18:26:31.814827Z\",\"iopub.status.idle\":\"2024-08-02T18:26:31.838667Z\",\"shell.execute_reply.started\":\"2024-08-02T18:26:31.814783Z\",\"shell.execute_reply\":\"2024-08-02T18:26:31.837253Z\"}}\ndf #layout of the entire dataset\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-08-02T18:26:31.842291Z\",\"iopub.execute_input\":\"2024-08-02T18:26:31.842836Z\",\"iopub.status.idle\":\"2024-08-02T18:26:31.870663Z\",\"shell.execute_reply.started\":\"2024-08-02T18:26:31.842794Z\",\"shell.execute_reply\":\"2024-08-02T18:26:31.868763Z\"}}\ndf.info()\n\n# %% [markdown]\n# This tells us that there are 150 columns of data with 5 entries each, with four of these columns (Sepal/Petal widths and lengths) containing floating-point numbers and the last column ```Species``` containing data of type ```object```, typically denoting arrays or strings.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-08-02T18:26:31.872439Z\",\"iopub.execute_input\":\"2024-08-02T18:26:31.874021Z\",\"iopub.status.idle\":\"2024-08-02T18:26:31.897430Z\",\"shell.execute_reply.started\":\"2024-08-02T18:26:31.873965Z\",\"shell.execute_reply\":\"2024-08-02T18:26:31.895840Z\"}}\ndf.nunique()\n\n# %% [markdown]\n# This tells us the number of unique entries in each column.\n\n# %% [markdown]\n# # Data Cleaning and Preparation\n# In this phase, the data is cleaned up and appropriately sorted for training. This is done by removing duplicate rows, missing values or irrelevant columns in the dataset. Then categorical variables in the dataset are encoded to enable information to be learnt from them, and the data is normalized if needed.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-08-02T18:26:31.899639Z\",\"iopub.execute_input\":\"2024-08-02T18:26:31.900094Z\",\"iopub.status.idle\":\"2024-08-02T18:26:31.912337Z\",\"shell.execute_reply.started\":\"2024-08-02T18:26:31.900047Z\",\"shell.execute_reply\":\"2024-08-02T18:26:31.910743Z\"}}\ndf.isna().sum()\n\n# %% [markdown]\n# Luckily there are no missing values in this dataset.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-08-02T18:26:31.913696Z\",\"iopub.execute_input\":\"2024-08-02T18:26:31.915128Z\",\"iopub.status.idle\":\"2024-08-02T18:26:31.931636Z\",\"shell.execute_reply.started\":\"2024-08-02T18:26:31.915089Z\",\"shell.execute_reply\":\"2024-08-02T18:26:31.930361Z\"}}\ndf.duplicated().sum()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-08-02T18:26:31.933322Z\",\"iopub.execute_input\":\"2024-08-02T18:26:31.934063Z\",\"iopub.status.idle\":\"2024-08-02T18:26:31.958963Z\",\"shell.execute_reply.started\":\"2024-08-02T18:26:31.934029Z\",\"shell.execute_reply\":\"2024-08-02T18:26:31.957754Z\"}}\ndf.drop_duplicates()\n\n# %% [markdown]\n# # Exploratory Data Analysis\n# This is the investigation and examination of data using data analysis and visualization techniques in order to gain insights and identify patterns. \n# \n# A good way to start can be to examine the statistical qualities of each individual variable(**Univariate analysis**), before proceeding to visualize the relationships between two or more variables, and each variable's relevance in predicting the response.(**Bivariate/Multivariate analysis**)\n# \n# For Univariate analysis, we can typically use:\n# 1. Histograms\n# 2. Box plots\n# 3. Count plots\n# 4. Bar charts\n# \n# And for Bivariate/multivariate analysis, we can either plot two or more variables against each other to investigate their relationship or use\n# 1. Pair plots\n# 2. Heatmaps\n# \n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-08-02T18:26:31.960611Z\",\"iopub.execute_input\":\"2024-08-02T18:26:31.962185Z\",\"iopub.status.idle\":\"2024-08-02T18:26:31.969641Z\",\"shell.execute_reply.started\":\"2024-08-02T18:26:31.962140Z\",\"shell.execute_reply\":\"2024-08-02T18:26:31.968150Z\"}}\ndf.rename({\"SepalLengthCm\":\"sepal_length\",\n            \"SepalWidthCm\":\"sepal_width\",\n            \"PetalLengthCm\":\"petal_length\",\n            \"PetalWidthCm\":\"petal_width\",\n            \"Species\":\"species\"}, axis=1, inplace=True)\n\n# %% [markdown]\n# Columns renamed for convenience\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-08-02T18:26:31.976060Z\",\"iopub.execute_input\":\"2024-08-02T18:26:31.976570Z\",\"iopub.status.idle\":\"2024-08-02T18:26:32.000104Z\",\"shell.execute_reply.started\":\"2024-08-02T18:26:31.976530Z\",\"shell.execute_reply\":\"2024-08-02T18:26:31.998603Z\"}}\ndf.head()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-08-02T18:26:32.002716Z\",\"iopub.execute_input\":\"2024-08-02T18:26:32.004163Z\",\"iopub.status.idle\":\"2024-08-02T18:26:33.463196Z\",\"shell.execute_reply.started\":\"2024-08-02T18:26:32.004114Z\",\"shell.execute_reply\":\"2024-08-02T18:26:33.461936Z\"}}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings \nwarnings.filterwarnings(\"ignore\")\n\n# %% [markdown]\n# UNIVARIATE ANALYSIS\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-08-02T18:26:33.465110Z\",\"iopub.execute_input\":\"2024-08-02T18:26:33.465592Z\",\"iopub.status.idle\":\"2024-08-02T18:26:33.913595Z\",\"shell.execute_reply.started\":\"2024-08-02T18:26:33.465550Z\",\"shell.execute_reply\":\"2024-08-02T18:26:33.912126Z\"}}\nplt.figure(figsize=(10,6))\nplt.hist(df['sepal_length'], bins=20, edgecolor=\"black\")\nplt.xlabel(\"Sepal Length\", fontsize=12)\nplt.ylabel(\"Frequency\", fontsize=12)\nplt.show()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-08-02T18:26:33.916499Z\",\"iopub.execute_input\":\"2024-08-02T18:26:33.917164Z\",\"iopub.status.idle\":\"2024-08-02T18:26:34.244509Z\",\"shell.execute_reply.started\":\"2024-08-02T18:26:33.917113Z\",\"shell.execute_reply\":\"2024-08-02T18:26:34.243240Z\"}}\nplt.figure(figsize=(10,6))\nplt.hist(df['petal_length'], bins=20, edgecolor=\"black\")\nplt.xlabel(\"Petal Length\", fontsize=12)\nplt.ylabel(\"Frequency\", fontsize=12)\nplt.show()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-08-02T18:26:34.246010Z\",\"iopub.execute_input\":\"2024-08-02T18:26:34.246855Z\",\"iopub.status.idle\":\"2024-08-02T18:26:34.591500Z\",\"shell.execute_reply.started\":\"2024-08-02T18:26:34.246790Z\",\"shell.execute_reply\":\"2024-08-02T18:26:34.590243Z\"}}\nplt.figure(figsize=(10,6))\nsns.boxplot(data=df)\nplt.title(\"Boxplot of Data\", fontsize=14)\nplt.xlabel(\"Features\", fontsize=12)\nplt.ylabel(\"Values\", fontsize=12)\nplt.show()\n\n# %% [markdown]\n# The boxplot can also be a useful tool in identifying outlier datapoints, such as those seen in the ```sepal_width``` column above. These are datapoints or observations which are numerically distant from the rest of our data. \n# \n# These observations can negatively affect the mean and standard deviation of our data, increasing error variance and producing models which give statistically inaccuracte results. Although there are many methods and techniques for dealing with outliers, simply removing them can also be considered a good option. \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-08-02T18:26:34.593386Z\",\"iopub.execute_input\":\"2024-08-02T18:26:34.593756Z\",\"iopub.status.idle\":\"2024-08-02T18:26:34.602138Z\",\"shell.execute_reply.started\":\"2024-08-02T18:26:34.593725Z\",\"shell.execute_reply\":\"2024-08-02T18:26:34.600988Z\"}}\ndef identify_and_drop_outliers(df, field_name):\n    #define the interquartile range (IQR) of the dataset column\n    iqr = 1.5 * (np.percentile(df[field_name], 75) - np.percentile(df[field_name], 25))\n    \n    #define outlier fences\n    upper_fence = iqr + np.percentile(df[field_name], 75)\n    lower_fence = np.percentile(df[field_name], 25) - iqr\n    \n    #Identify columns based on outlier fences\n    outliers = df[(df[field_name] < lower_fence) | (df[field_name] > upper_fence)]\n    print(f\"Identified Outliers:\\n {outliers}\")\n    \n    #drop the identified rows from the dataset\n    df.drop(df[df[field_name] > upper_fence].index, inplace=True)\n    df.drop(df[df[field_name] < lower_fence].index, inplace=True)\n\n# %% [markdown]\n# We define a function ```identify_and_drop_outliers``` that defines \"outlier fences\" using the interquartile range of the dataset column (values beyond the range of **±1.5 * IQR** are treated as outliers), prints the identified columns and removes them. We can then simply pass ```sepal_width``` to it, as well as reuse for any other functions which possess outliers.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-08-02T18:26:34.603820Z\",\"iopub.execute_input\":\"2024-08-02T18:26:34.604230Z\",\"iopub.status.idle\":\"2024-08-02T18:26:34.626126Z\",\"shell.execute_reply.started\":\"2024-08-02T18:26:34.604178Z\",\"shell.execute_reply\":\"2024-08-02T18:26:34.624914Z\"}}\nidentify_and_drop_outliers(df, \"sepal_width\")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-08-02T18:26:34.627626Z\",\"iopub.execute_input\":\"2024-08-02T18:26:34.628348Z\",\"iopub.status.idle\":\"2024-08-02T18:26:34.800302Z\",\"shell.execute_reply.started\":\"2024-08-02T18:26:34.628306Z\",\"shell.execute_reply\":\"2024-08-02T18:26:34.799114Z\"}}\n#Visualization of column after outlier removal\nsns.boxplot(x=df['sepal_width'])\nplt.xlim(1,5)\nplt.title(\"Box Plot after outlier removal\")\nplt.show()\n\n# %% [markdown]\n# Plotting the frequency distribution of all the independent variables\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-08-02T18:26:34.801630Z\",\"iopub.execute_input\":\"2024-08-02T18:26:34.802059Z\",\"iopub.status.idle\":\"2024-08-02T18:26:36.294038Z\",\"shell.execute_reply.started\":\"2024-08-02T18:26:34.802018Z\",\"shell.execute_reply\":\"2024-08-02T18:26:36.292728Z\"}}\nplt.figure(figsize=(10, 6))\nn_features = len(df.columns[:-1])\nn_rows = n_features // 2 + n_features % 2  # Calculate the number of rows needed\n\nfor i, feature in enumerate(df.columns[:-1]):\n    plt.subplot(n_rows, 2, i+1)  # Adjust subplot layout dynamically\n    sns.histplot(df[feature], kde=True)\n    plt.title(f'Distribution of {feature}', fontsize=12)\n    plt.xlabel(feature, fontsize=10)\n    plt.ylabel('Frequency', fontsize=10)\n\nplt.tight_layout()\nplt.show()\n\n# %% [markdown]\n# BIVARIATE ANALYSIS\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-08-02T18:26:36.295700Z\",\"iopub.execute_input\":\"2024-08-02T18:26:36.296054Z\",\"iopub.status.idle\":\"2024-08-02T18:26:36.696429Z\",\"shell.execute_reply.started\":\"2024-08-02T18:26:36.296025Z\",\"shell.execute_reply\":\"2024-08-02T18:26:36.695290Z\"}}\nplt.figure(figsize=(10, 6))\nsns.barplot(x='species', y='sepal_length', data=df)\nplt.title('Bar Plot of Sepal Length by Species', fontsize=14)\nplt.xlabel('Species', fontsize=12)\nplt.ylabel('Sepal Length', fontsize=12)\nplt.show()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-08-02T18:26:36.698198Z\",\"iopub.execute_input\":\"2024-08-02T18:26:36.698950Z\",\"iopub.status.idle\":\"2024-08-02T18:26:37.017329Z\",\"shell.execute_reply.started\":\"2024-08-02T18:26:36.698908Z\",\"shell.execute_reply\":\"2024-08-02T18:26:37.016001Z\"}}\nplt.figure(figsize=(8, 6))\nsns.barplot(x='species', y='petal_length', data=df)\nplt.title('Bar Plot of Petal Length by Species', fontsize=14)\nplt.xlabel('Species', fontsize=12)\nplt.ylabel('PetalLength', fontsize=12)\nplt.show()\n\n# %% [markdown]\n# We can also visualize the correlation matrix of all the independent variables.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-08-02T18:26:37.019132Z\",\"iopub.execute_input\":\"2024-08-02T18:26:37.019625Z\",\"iopub.status.idle\":\"2024-08-02T18:26:37.401489Z\",\"shell.execute_reply.started\":\"2024-08-02T18:26:37.019579Z\",\"shell.execute_reply\":\"2024-08-02T18:26:37.399787Z\"}}\nnumeric_columns =df.select_dtypes(include=['float64', 'int64'])\nplt.figure(figsize=(8, 4))\nsns.heatmap(numeric_columns.corr(), annot=True, linewidths=0.5, cmap=\"cubehelix_r\")\nplt.title('Correlation Heatmap of Iris Dataset', fontsize=14)\nplt.show()\n\n# %% [markdown]\n# PAIRPLOT OF THE DATASET\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-08-02T18:26:37.403308Z\",\"iopub.execute_input\":\"2024-08-02T18:26:37.403653Z\",\"iopub.status.idle\":\"2024-08-02T18:26:44.352722Z\",\"shell.execute_reply.started\":\"2024-08-02T18:26:37.403621Z\",\"shell.execute_reply\":\"2024-08-02T18:26:44.351588Z\"}}\nsns.pairplot(df, hue='species')\nplt.title('Pairplot of the Iris Dataset', fontsize=14)\nplt.show()\n\n# %% [markdown]\n# # Dataset Splitting\n# In this phase, we split the dataset into training and test splits (80/20) to enable proper statistical learning. However, we must first process the ```Species``` category of our dataset into a format that can be used by the model, as we do this by simply encoding the different classes of the response variable into numbers. \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-08-02T18:26:44.354428Z\",\"iopub.execute_input\":\"2024-08-02T18:26:44.354806Z\",\"iopub.status.idle\":\"2024-08-02T18:26:44.447250Z\",\"shell.execute_reply.started\":\"2024-08-02T18:26:44.354774Z\",\"shell.execute_reply\":\"2024-08-02T18:26:44.445878Z\"}}\nfrom sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\ndf['species'] = encoder.fit_transform(df['species'])\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-08-02T18:26:44.448694Z\",\"iopub.execute_input\":\"2024-08-02T18:26:44.449063Z\",\"iopub.status.idle\":\"2024-08-02T18:26:44.465655Z\",\"shell.execute_reply.started\":\"2024-08-02T18:26:44.449032Z\",\"shell.execute_reply\":\"2024-08-02T18:26:44.464251Z\"}}\ndf.head()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-08-02T18:26:44.467345Z\",\"iopub.execute_input\":\"2024-08-02T18:26:44.467718Z\",\"iopub.status.idle\":\"2024-08-02T18:26:44.584091Z\",\"shell.execute_reply.started\":\"2024-08-02T18:26:44.467687Z\",\"shell.execute_reply\":\"2024-08-02T18:26:44.582941Z\"}}\nfrom sklearn.model_selection import train_test_split\n\nX = df.iloc[:, :-1]\ny = df.iloc[:, -1]\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=2)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-08-02T18:26:44.585608Z\",\"iopub.execute_input\":\"2024-08-02T18:26:44.586004Z\",\"iopub.status.idle\":\"2024-08-02T18:26:44.592605Z\",\"shell.execute_reply.started\":\"2024-08-02T18:26:44.585973Z\",\"shell.execute_reply\":\"2024-08-02T18:26:44.591411Z\"}}\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)\n\n# %% [markdown]\n# # Model Training and Evaluation\n# \n# This is the crux of the machine learning process, where untrained models are fit to curated datasets to produce artifacts which can then be used to conduct inference and provide accurate predictions.\n# \n# After the models are trained, we can evaluate them with relevant metrics to ensure quality performance. We also utilize cross-validation, which splits the dataset into multiple folds to give a better assurance of the quality of the model.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-08-02T18:26:44.594387Z\",\"iopub.execute_input\":\"2024-08-02T18:26:44.594866Z\",\"iopub.status.idle\":\"2024-08-02T18:26:48.403952Z\",\"shell.execute_reply.started\":\"2024-08-02T18:26:44.594814Z\",\"shell.execute_reply\":\"2024-08-02T18:26:48.402438Z\"}}\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\n\naccuracy_scores = cross_val_score(RandomForestClassifier(), X, y, cv=5, scoring='accuracy')\nprecision_scores = cross_val_score(RandomForestClassifier(), X, y, cv=5, scoring='precision_macro')\nf1_scores = cross_val_score(RandomForestClassifier(), X, y, cv=5, scoring='f1_macro')\nprint(f\"Cross validation Accuracy scores: {accuracy_scores}\")\nprint(f\"Mean accuracy: {accuracy_scores.mean() * 100:.3f}%\\n\")\n\nprint(f\"Cross validation precision scores: {precision_scores}\")\nprint(f\"Cross Validaton Mean Score : {precision_scores.mean():.3f}\\n\")\n\nprint(f\"Cross validation F1 scores: {f1_scores}\")\nprint(f\"Mean f1 score: {f1_scores.mean():.3f}\\n\")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-08-02T18:26:48.409834Z\",\"iopub.execute_input\":\"2024-08-02T18:26:48.410282Z\",\"iopub.status.idle\":\"2024-08-02T18:26:48.842063Z\",\"shell.execute_reply.started\":\"2024-08-02T18:26:48.410237Z\",\"shell.execute_reply\":\"2024-08-02T18:26:48.840921Z\"}}\nfrom sklearn.linear_model import LogisticRegression\n\naccuracy_scores = cross_val_score(LogisticRegression(), X, y, cv=5, scoring='accuracy')\nprecision_scores = cross_val_score(LogisticRegression(), X, y, cv=5, scoring='precision_macro')\nf1_scores = cross_val_score(LogisticRegression(), X, y, cv=5, scoring='f1_macro')\nprint(f\"Cross validation Accuracy scores: {accuracy_scores}\")\nprint(f\"Mean accuracy: {accuracy_scores.mean() * 100:.3f}%\\n\")\n\nprint(f\"Cross validation precision scores: {precision_scores}\")\nprint(f\"Cross Validaton Mean Score : {precision_scores.mean():.3f}\\n\")\n\nprint(f\"Cross validation F1 scores: {f1_scores}\")\nprint(f\"Mean f1 score: {f1_scores.mean():.3f}\\n\")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-08-02T18:26:48.843308Z\",\"iopub.execute_input\":\"2024-08-02T18:26:48.843639Z\",\"iopub.status.idle\":\"2024-08-02T18:26:48.962624Z\",\"shell.execute_reply.started\":\"2024-08-02T18:26:48.843610Z\",\"shell.execute_reply\":\"2024-08-02T18:26:48.961170Z\"}}\nfrom sklearn.svm import SVC\n\nsvc = SVC()\n\n\naccuracy_scores = cross_val_score(svc, X, y, cv=5, scoring='accuracy')\nprecision_scores = cross_val_score(svc, X, y, cv=5, scoring='precision_macro')\nf1_scores = cross_val_score(svc, X, y, cv=5, scoring='f1_macro')\nprint(f\"Cross validation Accuracy scores: {accuracy_scores}\")\nprint(f\"Mean accuracy: {accuracy_scores.mean() * 100:.3f}%\\n\")\n\nprint(f\"Cross validation precision scores: {precision_scores}\")\nprint(f\"Cross Validaton Mean Score : {precision_scores.mean():.3f}\\n\")\n\nprint(f\"Cross validation F1 scores: {f1_scores}\")\nprint(f\"Mean f1 score: {f1_scores.mean():.3f}\\n\")\n\n# %% [markdown]\n# All our models give exceptional scores, so we can simply select any one of them for deployment.","metadata":{"_uuid":"fc300551-8787-4795-b43f-5b56e657a872","_cell_guid":"ded7848d-9888-4f11-aab6-c924c83f4242","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}